import px

def unnest_topics_and_partitions(df, body_field: str):
  '''
  Unnest the topics and partitions from a data frame. body_field is the target column to unnest,
  usually 'req_body' or 'resp'.
  '''
  # Get topic_name
  df.topics = px.pluck(df[body_field], 'topics')
  df = json_unnest_first5(df, 'topic', 'topics')
  df = df[df.topic != '']
  df.topic_name = px.pluck(df.topic, 'name')

  # Get partition_idx
  df.partitions = px.pluck(df.topic, 'partitions')
  df = json_unnest_first5(df, 'partition', 'partitions')
  df = df[df.partition != '']
  df.partition_idx = px.pluck(df.partition, 'index')

  # Get message_size
  df.message_set = px.pluck(df.partition, 'message_set')
  df.message_size = px.pluck(df.message_set, 'size')
  df.message_size = px.atoi(df.message_size, 0)
  return df


def json_unnest_first5(df, dest_col, src_col):
  '''Unnest the first 5 values in a JSON array in the src_col, and put it in the
  dest_col.
  '''
  df0 = json_array_index(df, dest_col, src_col, 0)
  df1 = json_array_index(df, dest_col, src_col, 1)
  df2 = json_array_index(df, dest_col, src_col, 2)
  df3 = json_array_index(df, dest_col, src_col, 3)
  df4 = json_array_index(df, dest_col, src_col, 4)
  df = df0.append(df1).append(df2).append(df3).append(df4)
  return df


def json_array_index(df, dest_col, src_col, idx):
  df[dest_col] = px.pluck_array(df[src_col], idx)
  return df


def select_columns(df):
  return df[[
      'time_', 'upid', 'req_cmd', 'client_id', 'latency',
      'source_pod', 'destination_pod', 'source_service', 'destination_service',
      'source_namespace', 'destination_namespace', 'req_body', 'resp',
      'topic_name', 'partition_idx', 'partition',
      'message_size', 'error_code',
  ]]


def get_produce_records(df):
  '''
  Get all the produce records and filter by a specified topic. If topic is empty, all
  produce records are retained.
  '''
  # Produce requests have command 0.
  producer_df = df[df.req_cmd == 0]

  producer_df = unnest_topics_and_partitions(producer_df, 'req_body')
  producer_df.req_partition_idx = df.partition_idx
  # Error code is always in the response.
  producer_df = unnest_topics_and_partitions(producer_df, 'resp')

  producer_df.error_code = px.pluck(producer_df.partition, 'error_code')
  producer_df = producer_df[producer_df.partition_idx == producer_df.req_partition_idx]
  return select_columns(producer_df)


def get_fetch_records(df):
  '''
  Get all the fetch records and filter by a specified topic. If topic is empty, all
  fetch records are retained.
  '''
  # Fetch requests have command 1.
  consumer_df = df[df.req_cmd == 1]

  consumer_df = unnest_topics_and_partitions(consumer_df, 'resp')
  consumer_df.error_code = px.pluck(consumer_df.partition, 'error_code')
  return select_columns(consumer_df)


def get_remaining_records(df):
  '''
  Get all the fetch records and filter by a specified topic. If topic is empty, all
  fetch records are retained.
  '''
  # Exclude Produce (cmd 0) and Fetch (cmd 1) commands.
  df = df[df.req_cmd > 1]
  df.topic_name = ''
  df.partition_idx = ''
  df.partition = ''
  df.message_size = 0
  df.error_code = '0'
  return select_columns(df)

def remove_ns_prefix(column):
  return px.replace('[a-z0-9\-]*/', column, '')

def add_source_dest_columns(df):
  df.pod = df.ctx['pod']
  df.namespace = df.ctx['namespace']

  # If remote_addr is a pod, get its name. If not, use IP address.
  df.ra_pod = px.pod_id_to_pod_name(px.ip_to_pod_id(df.remote_addr))
  df.ra_name = px.select(df.ra_pod != '', df.ra_pod, px.nslookup(df.remote_addr))

  df.is_server_tracing = df.trace_role == 2
  # Set client and server based on trace_role.
  df.source_pod = px.select(df.is_server_tracing, df.ra_name, df.pod)
  df.destination_pod = px.select(df.is_server_tracing, df.pod, df.ra_name)

  df.source_service = px.pod_name_to_service_name(df.source_pod)
  df.source_service = px.select(df.source_service != '', df.source_service, df.source_pod)
  df.destination_service = px.pod_name_to_service_name(df.destination_pod)
  df.destination_service = px.select(df.destination_service != '', df.destination_service, df.destination_pod)

  df.destination_namespace = px.pod_name_to_namespace(df.destination_pod)
  df.source_namespace = px.pod_name_to_namespace(df.source_pod)
  df.source_service = px.Service(remove_ns_prefix(df.source_service))
  df.destination_service = px.Service(remove_ns_prefix(df.destination_service))
  df.source_pod = remove_ns_prefix(df.source_pod)
  df.destination_pod = remove_ns_prefix(df.destination_pod)
  return df

df = px.DataFrame(table='kafka_events.beta', start_time=px.plugin.start_time, end_time=px.plugin.end_time)
df.namespace = df.ctx['namespace']

df = add_source_dest_columns(df)
producer_df = get_produce_records(df)
consumer_df = get_fetch_records(df)
remaining_df = get_remaining_records(df)
df = producer_df.append([consumer_df, remaining_df])

# Convert latency from ns units to ms units.
# Kafka/NATS/AMQP measure latency differently than the rest of the Pixie protocols.
df.start_time = df.time_
df.end_time = df.time_ + df.latency
df.latency = df.latency / (1000.0 * 1000.0)

# Get throughput by adding size of message_sets. Note that this is the total size of the
# message batches, not the total number of bytes sent or received.
df.has_error = df.error_code != 'kNone' and df.error_code != '0'
df.req_cmd = px.kafka_api_key_name(df.req_cmd)
df.pixie = "pixie"
df.cluster_id = px.vizier_id()
df.cluster_name = px.vizier_name()
df.span_name = df.req_cmd + '/' + df.topic_name

# Restrict number of results.
df = df.head(1500)
px.export(
  df,
  px.otel.Data(
      resource={
          'service.name': df.source_service,
          'service.instance.id': df.source_pod,
          'k8s.pod.name': df.source_pod,
          'k8s.namespace.name': df.source_namespace,
          'k8s.cluster.name': df.cluster_name,
          'px.cluster.id': df.cluster_id,
          'instrumentation.provider': df.pixie,
          'kafka.service.name': df.destination_service,
          'kafka.broker.pod': df.destination_pod,
          'kafka.namespace.name': df.destination_namespace,
      },
      data=[
          px.otel.trace.Span(
              name=df.span_name,
              start_time=df.start_time,
              end_time=df.end_time,
              kind=px.otel.trace.SPAN_KIND_SERVER,
              attributes={
                  "kafka.client_id": df.client_id,
                  "kafka.has_error": df.has_error,
                  "kafka.message_size": df.message_size,
                  "kafka.partition_idx": df.partition_idx,
                  "kafka.partition": df.partition,
                  "kafka.req_body": df.req_body,
                  "kafka.req_cmd": df.req_cmd,
                  "kafka.resp": df.resp,
                  "kafka.topic": df.topic_name,
              },
          )
      ],
  ),
)