import px

def remove_duplicate_traces(df):
  ''' Removes duplicate traces.

      For historical reasons, Pixie traces MySQL requests on both the client AND server side:
      https://github.com/pixie-io/pixie/blob/5e5598ac46f39219148a36468b5318b1466a92d4/src/stirling/source_connectors/socket_tracer/conn_tracker.cc#L639
  '''

  # Keep client-side traces if server is outside the cluster (can't be resolved to pod or svc)
  df.remote_pod_id = px.ip_to_pod_id(df.remote_addr)
  df.remote_service_id = px.ip_to_service_id(df.remote_addr)
  df.remote_outside_cluster = df.remote_pod_id == '' and df.remote_service_id == ''
  df_client_traces = df[df.trace_role == 1 and df.remote_outside_cluster]

  df_server_traces = df[df.trace_role == 2]
  df_server_traces.append(df_client_traces)
  return df_server_traces

def remove_ns_prefix(column):
  return px.replace('[a-z0-9\-]*/', column, '')

def add_source_dest_columns(df):
  df.pod = df.ctx['pod']
  df.namespace = df.ctx['namespace']

  # If remote_addr is a pod, get its name. If not, use IP address.
  df.ra_pod = px.pod_id_to_pod_name(px.ip_to_pod_id(df.remote_addr))
  df.ra_name = px.select(df.ra_pod != '', df.ra_pod, px.nslookup(df.remote_addr))

  df.is_server_tracing = df.trace_role == 2
  # Set client and server based on trace_role.
  df.source_pod = px.select(df.is_server_tracing, df.ra_name, df.pod)
  df.destination_pod = px.select(df.is_server_tracing, df.pod, df.ra_name)

  df.source_service = px.pod_name_to_service_name(df.source_pod)
  df.source_service = px.select(df.source_service != '', df.source_service, df.source_pod)
  df.destination_service = px.pod_name_to_service_name(df.destination_pod)
  df.destination_service = px.select(df.destination_service != '', df.destination_service, df.destination_pod)

  df.destination_namespace = px.pod_name_to_namespace(df.destination_pod)
  df.source_namespace = px.pod_name_to_namespace(df.source_pod)
  df.source_service = px.Service(remove_ns_prefix(df.source_service))
  df.destination_service = px.Service(remove_ns_prefix(df.destination_service))
  df.source_pod = remove_ns_prefix(df.source_pod)
  df.destination_pod = remove_ns_prefix(df.destination_pod)
  return df

df = px.DataFrame('mysql_events', start_time=px.plugin.start_time, end_time=px.plugin.end_time)
df = remove_duplicate_traces(df)
df = add_source_dest_columns(df)

df.latency = df.latency / (1000 * 1000)
df.req_bytes = px.length(df.req_body)
df.resp_bytes = px.length(df.resp_body)
df = df.groupby(['source_pod', 'source_service', 'source_namespace', 'destination_pod', 'destination_service',
                'destination_namespace', 'req_cmd', 'resp_status']).agg(
  latency_count=('latency', px.count),
  latency_min=('latency', px.min),
  latency_max=('latency', px.max),
  latency_sum=('latency', px.sum),
  time_=('time_', px.max),
  req_bytes=('req_bytes', px.sum),
  resp_bytes=('resp_bytes', px.sum),
)

df.req_cmd = px.mysql_command_name(df.req_cmd)
df.cluster_name = px.vizier_name()
df.cluster_id = px.vizier_id()
df.pixie = 'pixie'
df.db_system = 'mysql'

px.export(
df, px.otel.Data(
  resource={
    'service.name': df.source_service,
    'service.instance.id': df.source_pod,
    'k8s.namespace.name': df.source_namespace,
    'mysql.service.name': df.destination_service,
    'mysql.pod.name': df.destination_pod,
    'mysql.namespace.name': df.destination_namespace,
    'mysql.req_cmd': df.req_cmd,
    'mysql.resp_status': df.resp_status,
    'k8s.cluster.name': df.cluster_name,
    'px.cluster.id': df.cluster_id,
    'instrumentation.provider': df.pixie,
    'db.system': df.db_system,
  },
  data=[
    px.otel.metric.Summary(
        name='mysql.latency',
        count=df.latency_count,
        sum=df.latency_sum,
        unit='ms',
        quantile_values={
          0.0: df.latency_min,
          1.0: df.latency_max,
        },
    ),
    px.otel.metric.Gauge(
      name='mysql.req_bytes',
      description='',
      value=df.req_bytes,
    ),
    px.otel.metric.Gauge(
      name='mysql.resp_bytes',
      description='',
      value=df.resp_bytes,
    )
]
),
)